Thesis Project Datasets on LLMs in Test Case Generation

Description

This repository hosts datasets used in my thesis project, which investigates the utility of Large Language Models (LLMs), specifically GPT-3.5 and LLaMA2, in generating test cases. The focus is on comparing these two models based on several evaluation criteria: Relevance and Coverage, Correctness and Specificity, and Innovativeness and Creativity. The datasets include generated test cases that serve as a basis for this comparison.

Datasets:

GPT_generated_test_cases.csv:

Content: Contains test cases generated by GPT-3.5, designed to assess the model's ability to produce relevant and comprehensive scenarios that align with specified requirements.

Source: Generated through the provided python code simulating using GPT-3.5.

Purpose: To evaluate the effectiveness of GPT-3.5 in test case generation, focusing on Relevance, Coverage, and Correctness.

LLaMA2_generated_test_cases.csv:

Content: Includes test cases generated by LLaMA2, focusing on the model's capacity to deliver specific, correct, innovative, and creative test scenarios.

Source: Generated via the provided python code simulating employing LLaMA2.

Purpose: To compare LLaMA2's performance against GPT-3.5 in terms of Specificity, Innovativeness, and Creativity in test case production.
Usage

These datasets are intended for academic research and educational purposes within the scope of evaluating LLM capabilities in software testing. If using these datasets in your research or projects, please cite this repository.
